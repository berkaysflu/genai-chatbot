# --- [WinError 1114] DLL HATASI ZM ---
# Bu komut, torch veya datasets import edilmeden EN NCE 癟al覺mal覺d覺r.
# Sisteme "ak覺an DLL'leri (libiomp5md.dll) g繹rmezden gel" der.
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"
# --- ZM SONU ---

import streamlit as st
import google.generativeai as genai
from datasets import load_dataset  # Hugging Face veri setlerini y羹klemek i癟in
from dotenv import load_dotenv # API anahtar覺n覺 .env dosyas覺ndan okumak i癟in

# Lokal Embedding'e (Rota 2) geri d繹nd羹k
from langchain_community.embeddings import HuggingFaceEmbeddings #
from langchain_community.vectorstores import FAISS #
from langchain_text_splitters import RecursiveCharacterTextSplitter #
from langchain_core.documents import Document #
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
# --- 1. PROJE KURULUMU VE API ANAHTARI YKLEME ---

# .env dosyas覺n覺 tam yolunu bularak g羹venle y羹kle
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))
    dotenv_path = os.path.join(script_dir, ".env")
    if os.path.exists(dotenv_path):
        load_dotenv(dotenv_path=dotenv_path)
    else:
        print(f"HATA: .env dosyas覺 u yolda bulunamad覺: {dotenv_path}")
except Exception as e:
    st.error(f".env dosyas覺 y羹klenirken beklenmedik hata: {e}")
    st.stop()

# Google API anahtar覺n覺 al (SADECE "generation" i癟in, embedding i癟in deil)
try:
    api_key = os.environ["GOOGLE_API_KEY"]
    if not api_key:
        raise ValueError("GOOGLE_API_KEY .env dosyas覺nda bulunamad覺 veya bo.")
    genai.configure(api_key=api_key)
    print("API Anahtar覺 baar覺yla y羹klendi.") 
except (KeyError, ValueError) as e:
    st.error(f"HATA: GOOGLE_API_KEY ortam deikeni ayarlanmam覺. L羹tfen .env dosyan覺z覺 kontrol edin. Hata: {e}")
    st.stop() 

# Streamlit aray羹z羹n羹n bal覺覺n覺 ayarla
st.set_page_config(page_title="Psikoloji Bilgi Asistan覺", page_icon="")
st.title(" Psikoloji Bilgi Asistan覺")
st.caption("Bu chatbot, RAG mimarisi kullan覺larak gelitirilmitir ve yaln覺zca y羹klenen veri setindeki bilgilere dayanarak yan覺t verir.")


# --- 2. VER襤 SET襤N襤 YKLEME (ADIM 2) ---
try:
    with st.spinner("Psikoloji 'kitapl覺覺' (veri seti) y羹kleniyor..."):
        dataset_dict = load_dataset("Amod/mental_health_counseling_conversations")
        split_name = list(dataset_dict.keys())[0]
        dataset = dataset_dict[split_name]

    st.success(f"Veri seti baar覺yla y羹klendi! Toplam {len(dataset)} adet konuma bulundu.")

    # Verinin nas覺l g繹r羹nd羹羹n羹 kontrol etmek i癟in bir 繹rnek g繹ster
    st.subheader("Veri Setinden Bir rnek:")
    st.text_area("rnek Soru (Context)", dataset[0]['Context'], height=100)
    st.text_area("rnek Cevap (Response)", dataset[0]['Response'], height=200)

except Exception as e:
    st.error(f"Veri seti y羹klenirken bir hata olutu: {e}")
    # Bu, DLL hatas覺 ise burada g繹r羹necek
    st.stop()

# --- 3. VER襤Y襤 HAFIZAYA (VEKTR VER襤TABANI) YKLEME (LOKAL MODEL) ---

@st.cache_resource
def create_vector_db(_dataset): 
    with st.spinner("RAG 'haf覺zas覺' (Lokal Model) oluturuluyor... (Bu ilem 5-10 dakika s羹rebilir)"):

        # 1. Veriyi Haz覺rla
        documents = []
        for item in _dataset: 
            doc = Document(page_content=item['Response'])
            documents.append(doc)

        # 2. Metinleri Par癟ala
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = text_splitter.split_documents(documents)

        # 3. LOKAL Embedding Modeli'ni Y羹kle (API YOK)
        # Bu model, API'ye gitmek yerine kendi bilgisayar覺nda 癟al覺覺r.
        model_name = "sentence-transformers/all-MiniLM-L6-v2"
        embeddings = HuggingFaceEmbeddings(model_name=model_name)

        # 4. FAISS Vekt繹r Veritaban覺'n覺 Olutur
        st.info(f"Toplam {len(chunks)} metin par癟as覺 (chunk) bulundu. Embedding ilemi bal覺yor...")
        vector_db = FAISS.from_documents(chunks, embeddings)
        return vector_db

# Haf覺za fonksiyonunu 癟a覺r
try:
    vector_db = create_vector_db(dataset)
    st.success("RAG 'haf覺zas覺' baar覺yla oluturuldu ve y羹klendi!")
except Exception as e:
    st.error(f"RAG haf覺zas覺 oluturulurken bir hata olutu: {e}")
    st.stop()




# --- 4. CHATBOT ARAYZ VE RAG PIPELINE ---

st.header(" Chatbot ile Konuun")

# Streamlit'in session_state 繹zelliini kullanarak sohbet ge癟miini tut
if "messages" not in st.session_state:
    st.session_state.messages = []

# Sohbet ge癟miini ekranda g繹ster
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Kullan覺c覺dan girdi al
if prompt := st.chat_input("Psikoloji ile ilgili sorunuzu buraya yaz覺n..."):
    # Kullan覺c覺n覺n mesaj覺n覺 sohbet ge癟miine ekle ve ekranda g繹ster
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # RAG Pipeline'覺n覺 al覺t覺r
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # 1. Retrieval (FAISS Haf覺zas覺ndan Bilgi Getirme)
            # vector_db'yi bir retriever objesine d繹n羹t羹r (en benzer 3 cevab覺 getirsin)
            retriever = vector_db.as_retriever(search_kwargs={"k": 5})
            
            # 2. Prompt Tasar覺m覺 (Gemini'ye Nas覺l Soraca覺m覺z)
            # Haf覺zadan gelen bilgileri ({context}) ve kullan覺c覺n覺n sorusunu ({question}) alacak bir ablon
            template = """Aa覺daki balam覺 kullanarak soruyu yan覺tlamaya 癟al覺覺n. 
                Eer balamda yeterli bilgi yoksa, genel bilginizi kullanarak k覺sa bir cevap verin 
                ancak cevab覺n genel bilgiye dayand覺覺n覺 belirtin. Cevap uydurmaktan ka癟覺n覺n.

                Balam: {context}

                Soru: {question}

                Yard覺mc覺 Cevap:"""
            prompt_template = ChatPromptTemplate.from_template(template)

            # 3. Generation (Gemini Modeli ile Cevap retme)
            llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0.7)

            # 4. LangChain Expression Language (LCEL) ile RAG Pipeline'覺n覺 Oluturma
            # Bu zincir:
            #   - Kullan覺c覺n覺n sorusunu al覺r ({question})
            #   - Retriever ile ilgili belgeleri bulur ve {context}'e atar
            #   - Prompt ablonunu doldurur
            #   - LLM (Gemini) ile cevap 羹retir
            #   - Cevab覺 metin olarak formatlar (StrOutputParser)
            rag_chain = (
                {"context": retriever, "question": RunnablePassthrough()} 
                | prompt_template 
                | llm 
                | StrOutputParser() 
            )

            # Pipeline'覺 癟al覺t覺r ve cevab覺 al (Streamlit'in streaming'i ile uyumlu deil, o y羹zden direkt invoke)
            full_response = rag_chain.invoke(prompt)
            message_placeholder.markdown(full_response)

        except Exception as e:
            st.error(f"Cevap 羹retilirken bir hata olutu: {e}")
            full_response = "zg羹n羹m, bir hata olutu."
            message_placeholder.markdown(full_response)

    # Botun cevab覺n覺 sohbet ge癟miine ekle
    st.session_state.messages.append({"role": "assistant", "content": full_response})