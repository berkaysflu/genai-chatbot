# --- [WinError 1114] DLL HATASI ZM ---
# Bu komut, torch veya datasets import edilmeden EN NCE 癟al覺mal覺d覺r.
# Sisteme "ak覺an DLL'leri (libiomp5md.dll) g繹rmezden gel" der.
# Bu, 繹zellikle sentence-transformers gibi torch kullanan k羹t羹phanelerin
# Windows'ta neden olduu yayg覺n bir sorunu 癟繹zer.
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"
# --- ZM SONU ---

import streamlit as st
import google.generativeai as genai
from datasets import load_dataset  # Hugging Face veri setlerini y羹klemek i癟in
from dotenv import load_dotenv # API anahtar覺n覺 .env dosyas覺ndan okumak i癟in
import sys # DLL 癟繹z羹m羹 i癟in sys.prefix kullan覺ld覺 (art覺k kullan覺lm覺yor ama import kalabilir)
import time # Embedding s覺ras覺nda gecikme eklemek i癟in (art覺k kullan覺lm覺yor ama import kalabilir)

# Lokal Embedding (a癟覺k kaynak model) i癟in gerekli importlar
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# RAG Pipeline ve Generation i癟in gerekli importlar
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# --- 1. PROJE KURULUMU VE API ANAHTARI YKLEME ---

# .env dosyas覺n覺, script'in bulunduu dizinden tam yolunu bularak y羹kle
# Bu, uygulaman覺n farkl覺 ortamlarda 癟al覺t覺r覺ld覺覺nda .env dosyas覺n覺 bulmas覺n覺 garantiler.
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))
    dotenv_path = os.path.join(script_dir, ".env")
    if os.path.exists(dotenv_path):
        load_dotenv(dotenv_path=dotenv_path)
        print(f".env dosyas覺 tam yoldan y羹klendi: {dotenv_path}")
    else:
        # Hata durumunda aa覺daki try blou bunu yakalayacak.
        print(f"UYARI: .env dosyas覺 u yolda bulunamad覺: {dotenv_path}")
except Exception as e:
    st.error(f".env dosyas覺 y羹klenirken beklenmedik hata: {e}")
    st.stop()

# Google API anahtar覺n覺 ortam deikenlerinden al (Sadece Generation i癟in)
try:
    api_key = os.environ["GOOGLE_API_KEY"]
    if not api_key:
        raise ValueError("GOOGLE_API_KEY .env dosyas覺nda bulunamad覺 veya bo.")
    genai.configure(api_key=api_key)
    print("Google API Anahtar覺 baar覺yla y羹klendi.")
except (KeyError, ValueError) as e:
    st.error(f"HATA: GOOGLE_API_KEY ortam deikeni ayarlanmam覺. L羹tfen .env dosyan覺z覺 kontrol edin. Hata: {e}")
    st.stop()

# --- STREAMLIT ARAYZ AYARLARI ---
st.set_page_config(page_title="Psikoloji Bilgi Asistan覺", page_icon="")
st.title(" Psikoloji Bilgi Asistan覺")
st.caption("Bu chatbot, RAG mimarisi kullan覺larak gelitirilmitir ve yaln覺zca y羹klenen veri setindeki bilgilere dayanarak yan覺t verir.")


# --- 2. VER襤 SET襤N襤 YKLEME ---
# Hugging Face'ten psikoloji dan覺manl覺k veri setini y羹kle
# Bu veri seti, RAG sistemimizin bilgi kayna覺 (knowledge base) olacakt覺r.
@st.cache_data # Veri setini 繹nbellee al, her seferinde indirme
def load_data():
    try:
        dataset_dict = load_dataset("Amod/mental_health_counseling_conversations")
        # Veri setindeki ilk b繹l羹m羹 (split) al, genellikle 'train' olur.
        split_name = list(dataset_dict.keys())[0]
        return dataset_dict[split_name]
    except Exception as e:
        st.error(f"Veri seti y羹klenirken bir hata olutu: {e}")
        st.stop()

dataset = load_data()
st.success(f"Veri seti baar覺yla y羹klendi! Toplam {len(dataset)} adet konuma bulundu.")

# Verinin yap覺s覺n覺 g繹stermek i癟in aray羹zde bir 繹rnek g繹ster (opsiyonel)
with st.expander("Veri Setinden Bir rnek G繹ster"):
    st.text_area("rnek Soru (Context)", dataset[0]['Context'], height=100)
    st.text_area("rnek Cevap (Response)", dataset[0]['Response'], height=200)

# --- 3. RAG HAFIZASINI (VEKTR VER襤TABANI) OLUTURMA ---
# Bu fonksiyon, veri setindeki cevaplar覺 al覺r, par癟alar, vekt繹rlere d繹n羹t羹r羹r
# ve FAISS veritaban覺na y羹kler. Lokal embedding modeli kullan覺l覺r.
@st.cache_resource # Hesaplanan sonucu (vector_db) 繹nbellee al, tekrar hesaplama
def create_vector_db(_dataset):
    with st.spinner("RAG 'haf覺zas覺' (Lokal Model) oluturuluyor... (Bu ilem ilk 癟al覺t覺rmada 5-10 dakika s羹rebilir)"):

        # 1. Veriyi Haz覺rla: Sadece uzman cevaplar覺n覺 ('Response') al
        documents = [Document(page_content=item['Response']) for item in _dataset if item['Response']] # Bo cevaplar覺 atla

        # 2. Metinleri Par癟ala (Chunking): Uzun metinleri y繹netilebilir par癟alara b繹l
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = text_splitter.split_documents(documents)

        if not chunks:
             st.error("Veri setinden ge癟erli metin par癟as覺 (chunk) bulunamad覺. L羹tfen veri setini kontrol edin.")
             st.stop()

        # 3. LOKAL Embedding Modeli'ni Y羹kle: Metinleri vekt繹rlere d繹n羹t羹r
        # A癟覺k kaynakl覺 `all-MiniLM-L6-v2` modeli kullan覺l覺r, API gerektirmez.
        try:
            model_name = "sentence-transformers/all-MiniLM-L6-v2"
            embeddings = HuggingFaceEmbeddings(model_name=model_name)
        except Exception as e:
            st.error(f"Lokal embedding modeli y羹klenirken hata olutu ({model_name}): {e}")
            st.stop()

        # 4. FAISS Vekt繹r Veritaban覺'n覺 Olutur: Vekt繹rleri h覺zl覺 arama i癟in indeksle
        st.info(f"Toplam {len(chunks)} metin par癟as覺 (chunk) bulundu. Embedding ilemi bal覺yor...")
        try:
            vector_db = FAISS.from_documents(chunks, embeddings)
            return vector_db
        except Exception as e:
            st.error(f"FAISS veritaban覺 oluturulurken hata: {e}")
            st.stop()


# Haf覺za fonksiyonunu 癟a覺r ve sonucu al
try:
    vector_db = create_vector_db(dataset)
    st.success("RAG 'haf覺zas覺' baar覺yla oluturuldu ve y羹klendi!")
except Exception as e:
    # create_vector_db i癟indeki hatalar zaten st.stop() ile durdurur,
    # ama yine de genel bir hata yakalama ekleyelim.
    st.error(f"RAG haf覺zas覺 oluturma s羹recinde genel bir hata olutu: {e}")
    st.stop()


# --- 4. CHATBOT ARAYZ VE RAG CEVAP RET襤M襤 ---

st.header(" Chatbot ile Konuun")

# Streamlit'in session_state 繹zelliini kullanarak sohbet ge癟miini oturumlar aras覺nda tut
if "messages" not in st.session_state:
    st.session_state.messages = []

# Ge癟mi mesajlar覺 ekranda g繹ster
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Kullan覺c覺dan yeni bir soru al
if user_prompt := st.chat_input("Psikoloji ile ilgili sorunuzu buraya yaz覺n..."):
    # Kullan覺c覺n覺n sorusunu ge癟mie ekle ve ekranda g繹ster
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    # Botun cevab覺n覺 羹retmek i癟in RAG Pipeline'覺n覺 癟al覺t覺r
    with st.chat_message("assistant"):
        message_placeholder = st.empty() # Cevap alan覺 i癟in yer tutucu
        full_response = ""

        try:
            # 1. Retrieval: Kullan覺c覺n覺n sorusuna en benzer dok羹manlar覺 FAISS'ten bul
            # 'k=5' -> en benzer 5 dok羹man覺 getirir. Bu deer ayarlanabilir.
            retriever = vector_db.as_retriever(search_kwargs={"k": 5})

            # 2. Prompt ablonu: LLM'e (Gemini) nas覺l cevap vermesi gerektiini s繹yleyen talimat
            template = """Aa覺daki balam覺 kullanarak soruyu yan覺tlamaya 癟al覺覺n.
                        Eer balamda yeterli bilgi yoksa, genel bilginizi kullanarak k覺sa bir cevap verin
                        ancak cevab覺n genel bilgiye dayand覺覺n覺 belirtin. Cevap uydurmaktan ka癟覺n覺n.

                        Balam: {context}

                        Soru: {question}

                        Yard覺mc覺 Cevap:"""
            prompt_template = ChatPromptTemplate.from_template(template)

            # 3. LLM (Generation Model): Cevab覺 羹retecek olan model (Gemini Pro)
            # 'temperature' 羹retilen cevab覺n ne kadar "yarat覺c覺" olaca覺n覺 kontrol eder (0 = en deterministik).
            llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0.7)

            # 4. RAG Zinciri (LCEL - LangChain Expression Language):
            # Ad覺m 1, 2 ve 3'羹 birbirine balayan ak覺.
            rag_chain = (
                {"context": retriever, "question": RunnablePassthrough()} # Soruyu al, balam覺 bul
                | prompt_template                                         # Talimat覺 doldur
                | llm                                                     # Cevab覺 羹ret
                | StrOutputParser()                                       # Cevab覺 metne 癟evir
            )

            # 5. Zinciri al覺t覺r: Kullan覺c覺n覺n sorusunu zincire g繹nder ve cevab覺 al
            full_response = rag_chain.invoke(user_prompt)
            message_placeholder.markdown(full_response) # Cevab覺 ekrana yazd覺r

        except Exception as e:
            # Hata durumunda kullan覺c覺ya bilgi ver
            st.error(f"Cevap 羹retilirken bir hata olutu: {e}")
            full_response = "zg羹n羹m, u anda bir sorun ya覺yorum. L羹tfen daha sonra tekrar deneyin."
            message_placeholder.markdown(full_response)

    # Botun cevab覺n覺 (veya hata mesaj覺n覺) sohbet ge癟miine ekle
    st.session_state.messages.append({"role": "assistant", "content": full_response})